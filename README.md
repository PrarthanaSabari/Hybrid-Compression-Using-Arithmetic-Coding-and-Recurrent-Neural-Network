# ğŸš€ Hybrid Data Compression Using Arithmetic Coding & Recurrent Neural Networks (RNN)

> **A hybrid lossless compression framework that integrates LSTM-based neural probability modeling with Arithmetic Coding to achieve significantly higher compression efficiency compared to the traditional Shannonâ€“Fano algorithm.**

This repository contains all implementation files, notebooks, reports, and presentations for the ADCT (Advanced Data Compression Techniques) course project.

---

## â­ Project Highlights

### ğŸ”¥ Why a Hybrid Model?

Traditional compression methods like **Shannonâ€“Fano** use static probabilities, assigning codes based only on global symbol frequency. This is inefficient for data where patterns are contextual.

This project overcomes these limitations by using an **RNN (LSTM)** to predict adaptive, context-based probabilities for each symbol. These highly accurate, dynamic probabilities are then fed into an **Arithmetic Coder**, which can achieve near-optimal, fractional-bit compression.

### ğŸ“‰ Results at a Glance

The hybrid approach achieves approximately **4 times better compression** than the baseline Shannonâ€“Fano algorithm on the test data.

| Method | Original Size | Compressed Size | Compression Ratio | Size Reduction |
| :--- | :--- | :--- | :--- | :--- |
| **Shannonâ€“Fano** | 6254 bytes | 3563 bytes | 0.570Ã— | 43.0% |
| **RNN + Arithmetic (Proposed)** | 6254 bytes | **900 bytes** | **0.144Ã—** | **85.6%** |

---

## ğŸ§  System Overview

1.  **Input & Preprocessing**: The input file is read as raw bytes and converted into a tensor for the neural network.

2.  **RNN Probability Modeling**: An LSTM network processes the byte sequence, learns byte-level dependencies, and predicts the probability distribution for the *next* byte.

3.  **Softmax Conversion**: The raw output (logits) from the RNN is converted into a valid probability distribution using the Softmax function:
    $$P(x) = \frac{e^{z(x)}}{\sum_{k} e^{z(k)}}$$

4.  **Arithmetic Encoding**: The Arithmetic Encoder uses the adaptive probabilities from the RNN to progressively shrink a numerical interval for the entire file, resulting in a single, highly compact compressed output.

5.  **Arithmetic Decoding**: The decoder uses an identical, synchronized RNN model to recompute the *exact same* probability predictions during decompression, allowing it to perfectly reconstruct the original file from the compressed data.

6.  **Verification**: **SHA-256** hashes of the original and restored files are compared to confirm the compression is 100% lossless.

---

## ğŸ“‚ Repository Contents

```bash
.
â”œâ”€â”€ ADCT_REPORT.pdf
â”‚      (Complete project documentation with methodology,
â”‚       architecture, implementation and experimental results)
â”‚
â”œâ”€â”€ Abstract.pdf
â”‚      (One-page summary of the project)
â”‚
â”œâ”€â”€ Existing_system Shannon Fano.ipynb
â”‚      (Implementation of Shannonâ€“Fano for comparison)
â”‚
â”œâ”€â”€ Proposed system(Hybrid Compression).ipynb
â”‚      (Full hybrid system: RNN + Arithmetic Coding + UI)
â”‚
â”œâ”€â”€ Final review ppt.pptx
â”‚      (Final presentation slides)
â”‚
â”œâ”€â”€ Review -1 PPT ADCT.pptx
â”‚      (Review-1 presentation slides)
â”‚
â””â”€â”€ README.md
       (This file)

âš™ï¸ Features
âœ”ï¸ Complete Lossless Compression: Guarantees perfect reconstruction of the original file.

âœ”ï¸ Neural Probability Model: Uses an LSTM-based RNN for adaptive, context-aware probability prediction.

âœ”ï¸ Custom Coder: Includes full implementations of the Arithmetic Encoder and Decoder from scratch.

âœ”ï¸ Interactive UI: Features a Google Colab file upload and download interface.

âœ”ï¸ Comparative Analysis: Directly compares results against a baseline Shannonâ€“Fano implementation.

âœ”ï¸ Verification: Integrated SHA-256 hashing to verify lossless reconstruction.

ğŸ› ï¸ Setup & Running
â–¶ï¸ Run in Google Colab (Recommended)
Open Proposed system(Hybrid Compression).ipynb in Google Colab.

Upload any file (e.g., .txt, .py) when prompted by the file upload cell.

Run all cells in order.

The output will display compression statistics and provide UI buttons to download the compressed file and the restored (decompressed) file.

â–¶ï¸ Run Locally
1. Ensure you have Python and Jupyter Notebook installed.

2. Install the required dependencies:

pip install torch numpy

3. Launch Jupyter Notebook:

Launch Jupyter Notebook:

4. Open and run the Proposed system(Hybrid Compression).ipynb notebook.

ğŸ“Š Results & Comparison
Example Output (from Colab)
Original Size: 6254 bytes
Compressed Size: 900 bytes
Compression Ratio: 0.144Ã—

SHA-256 (original): c3a2...b91f
SHA-256 (restored): c3a2...b91f

Reconstruction: Verified lossless

ğŸ“ˆ Advantages
Learns Structure: The RNN model can learn long-term dependencies and complex structures in the data, unlike static models.

Accurate Probabilities: Produces highly accurate, context-specific probabilities, which is the key to high compression.

Near-Optimal: Arithmetic coding achieves compression ratios very close to the theoretical limit (entropy) defined by the probability model.

Universal: Works well on various file types, especially those with repetitive or complex patterns (like text, code, or structured data).

ğŸš§ Future Enhancements
Model Architecture: Replace the RNN/LSTM with more modern architectures like GRU or a Transformer-based model for potentially better probability modeling.

Speed & Optimization: Improve training speed and implement batched inference for faster encoding/decoding.

Standalone Tool: Build a standalone desktop application (e.g., using PyQT or Tkinter) for easier use.

Data Types: Extend the framework to specialize in other data types, such as images, audio, or video.

Model Quantization: Apply model compression (e.g., quantization) for deployment on low-resource or mobile devices.

ğŸ“š References
Shannonâ€“Fano Coding Theory

Arithmetic Coding Theory

Recurrent Neural Networks (RNNs) & LSTMs

PyTorch Documentation
